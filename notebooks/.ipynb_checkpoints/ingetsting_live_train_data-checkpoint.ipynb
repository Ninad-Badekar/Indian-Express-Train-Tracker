{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "125205ed-c64c-43d8-a737-e54f3e6a6c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DoubleType, IntegerType,\n",
    "    TimestampType, ArrayType\n",
    ")\n",
    "from pyspark.sql.functions import col, to_timestamp, lit, coalesce\n",
    "import psycopg2\n",
    "from pprint import pprint\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "626f138b-1ba6-4c6a-b843-6573580c35be",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"IndianRailwayTrainTracker\").master(\"local[*]\").config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.4\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1c5b3af4-2e73-4feb-8687-64c0183cc071",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"http://irctc-connect-main:3001/api/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7bca1a05-7893-47f4-9398-1cf722a9bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_info(train_number: str):\n",
    "    \"\"\"Fetch train info and route.\"\"\"\n",
    "    url = f\"{BASE_URL}/trainInfo\"\n",
    "    params = {\"trainNumber\": train_number}\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        print(\"\\n===== TRAIN INFO =====\")\n",
    "        print(f\"Train Number: {train_number}\")\n",
    "        if data.get(\"success\"):\n",
    "            train_info = data[\"data\"].get(\"trainInfo\", {})\n",
    "            route = data[\"data\"].get(\"route\", [])\n",
    "            print(f\"Name        : {train_info.get('name', 'N/A')}\")\n",
    "            print(f\"Type        : {train_info.get('type', 'N/A')}\")\n",
    "            print(\"\\n--- ROUTE ---\")\n",
    "            for stop in route:\n",
    "                print(f\"  {stop}\")\n",
    "        else:\n",
    "            print(\"Error:\", data.get(\"error\"))\n",
    "        print(\"\\n--- RAW RESPONSE ---\")\n",
    "        print(json.dumps(data, indent=4))\n",
    "        return data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching train info: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4f115d53-5868-48de-8ed2-502d97e41cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_train(train_number: str, date_str: str):\n",
    "    url = f\"{BASE_URL}/trackTrain\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"trainNumber\": train_number,\n",
    "        \"date\": date_str\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        print(\"\\n===== LIVE TRAIN STATUS =====\")\n",
    "        print(f\"Train Number : {train_number}\")\n",
    "        print(f\"Date         : {date_str}\")\n",
    "        print(f\"Last Updated : {datetime.now().strftime('%d-%m-%Y %H:%M:%S')}\")\n",
    "        print(\"\\n--- RAW RESPONSE ---\")\n",
    "        print(json.dumps(data, indent=4))\n",
    "        if isinstance(data, dict):\n",
    "            if \"current_station\" in data:\n",
    "                print(\"\\nCurrent Station:\", data[\"current_station\"])\n",
    "            if \"status\" in data:\n",
    "                print(\"Status:\", data[\"status\"])\n",
    "            if \"upcoming_stations\" in data:\n",
    "                print(\"\\nUpcoming Stations:\")\n",
    "                for stn in data[\"upcoming_stations\"]:\n",
    "                    print(f\"  - {stn}\")\n",
    "        return data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching train status: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b7e6591-e4b3-443d-9998-8850b583a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pnr_status(pnr: str):\n",
    "    \"\"\"Check PNR status.\"\"\"\n",
    "    url = f\"{BASE_URL}/checkPNRStatus\"\n",
    "    params = {\"pnr\": pnr}\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        print(\"\\n===== PNR STATUS =====\")\n",
    "        print(f\"PNR Number  : {pnr}\")\n",
    "        if data.get(\"success\"):\n",
    "            print(\"Train Name  :\", data[\"data\"][\"train\"][\"name\"])\n",
    "            print(\"From        :\", data[\"data\"][\"journey\"][\"from\"][\"name\"])\n",
    "            print(\"To          :\", data[\"data\"][\"journey\"][\"to\"][\"name\"])\n",
    "            print(\"Status      :\", data[\"data\"][\"status\"])\n",
    "        else:\n",
    "            print(\"Error:\", data.get(\"error\"))\n",
    "        print(\"\\n--- RAW RESPONSE ---\")\n",
    "        print(json.dumps(data, indent=4))\n",
    "        return data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching PNR status: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9560a57-5538-4d79-9e8f-5987ffc2d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def live_at_station(stn_code: str):\n",
    "    \"\"\"Get live upcoming trains at a station.\"\"\"\n",
    "    url = f\"{BASE_URL}/liveAtStation\"\n",
    "    params = {\"stnCode\": stn_code}\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        print(\"\\n===== LIVE AT STATION =====\")\n",
    "        print(f\"Station Code: {stn_code}\")\n",
    "        if data.get(\"success\"):\n",
    "            trains = data.get(\"data\", [])\n",
    "            print(f\"Upcoming Trains: {len(trains)}\")\n",
    "            for t in trains:\n",
    "                print(f\"  Train: {t}\")\n",
    "        else:\n",
    "            print(\"Error:\", data.get(\"error\"))\n",
    "        print(\"\\n--- RAW RESPONSE ---\")\n",
    "        print(json.dumps(data, indent=4))\n",
    "        return data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching live station data: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1faa673c-9312-4bc1-814c-d646d5157831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_train(train_number: str, date_str: str):\n",
    "    \"\"\"Track live train status.\"\"\"\n",
    "    url = f\"{BASE_URL}/trackTrain\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\"trainNumber\": train_number, \"date\": date_str}\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        # print(\"\\n===== LIVE TRAIN STATUS =====\")\n",
    "        # print(f\"Train Number : {train_number}\")\n",
    "        # print(f\"Date         : {date_str}\")\n",
    "        # print(f\"Last Updated : {datetime.now().strftime('%d-%m-%Y %H:%M:%S')}\")\n",
    "\n",
    "        # if isinstance(data, dict) and data.get(\"success\"):\n",
    "        #     print(\"Status:\", data.get(\"data\", {}).get(\"status\", \"N/A\"))\n",
    "        # elif isinstance(data, dict):\n",
    "        #     print(\"Error:\", data.get(\"error\"))\n",
    "\n",
    "        # print(\"\\n--- RAW RESPONSE ---\")\n",
    "        # print(json.dumps(data, indent=4))\n",
    "        return data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching train status: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05521731-22eb-4091-a2b1-6d88ddf35b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_train_between_stations(from_stn_code: str, to_stn_code: str):\n",
    "    \"\"\"Search trains between two stations.\"\"\"\n",
    "    url = f\"{BASE_URL}/searchTrainBetweenStations\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\"fromStnCode\": from_stn_code, \"toStnCode\": to_stn_code}\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        print(\"\\n===== SEARCH BETWEEN STATIONS =====\")\n",
    "        print(f\"From: {from_stn_code}  To: {to_stn_code}\")\n",
    "        if data.get(\"success\"):\n",
    "            trains = data.get(\"data\", [])\n",
    "            print(f\"Found {len(trains)} trains\")\n",
    "        else:\n",
    "            print(\"Error:\", data.get(\"error\"))\n",
    "        print(\"\\n--- RAW RESPONSE ---\")\n",
    "        print(json.dumps(data, indent=4))\n",
    "        return data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error searching trains: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e07f3b-53bb-4afd-9c11-91392de1518f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import (\n",
    "    create_engine, MetaData, Table, Column, Integer, String, DateTime, Float,\n",
    "    ForeignKey, Index\n",
    ")\n",
    "from sqlalchemy.dialects.postgresql import ARRAY\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "\n",
    "def create_table_dynamic(db_url: str, table_name: str, columns: list, indexes: list = None):\n",
    "    \"\"\"\n",
    "    Dynamically creates a table with optional indexes and foreign keys.\n",
    "    Reflects existing tables from DB first, so foreign keys can reference them.\n",
    "\n",
    "    Args:\n",
    "        db_url (str): Database connection string.\n",
    "        table_name (str): Name of the table to create.\n",
    "        columns (list): List of dicts with column metadata.\n",
    "                        Supported keys: name, type, length, primary_key, nullable,\n",
    "                                        autoincrement, unique, default, foreign_key.\n",
    "        indexes (list): List of dicts for indexes.\n",
    "    \"\"\"\n",
    "    engine = create_engine(db_url)\n",
    "    metadata = MetaData()\n",
    "\n",
    "    # Reflect all existing tables from DB into metadata\n",
    "    metadata.reflect(bind=engine)\n",
    "    # Now metadata.tables contains all existing tables known to DB\n",
    "\n",
    "    type_map = {\n",
    "        \"integer\": Integer,\n",
    "        \"string\": lambda length=255: String(length),\n",
    "        \"datetime\": DateTime,\n",
    "        \"float\": Float,\n",
    "        \"array\": lambda: ARRAY(Integer)  # Only integer arrays supported here\n",
    "    }\n",
    "\n",
    "    column_objs = []\n",
    "    for col in columns:\n",
    "        col_type_key = col[\"type\"].lower()\n",
    "        if col_type_key not in type_map:\n",
    "            raise ValueError(f\"Unsupported column type: {col['type']}\")\n",
    "\n",
    "        if col_type_key == \"string\":\n",
    "            col_type = type_map[col_type_key](col.get(\"length\", 255))\n",
    "        elif col_type_key == \"array\":\n",
    "            col_type = type_map[col_type_key]()  # no length param for ARRAY\n",
    "        else:\n",
    "            col_type = type_map[col_type_key]\n",
    "\n",
    "        fk = ForeignKey(col[\"foreign_key\"]) if \"foreign_key\" in col else None\n",
    "\n",
    "        args = [col[\"name\"], col_type]\n",
    "        if fk:\n",
    "            args.append(fk)\n",
    "\n",
    "        column_objs.append(\n",
    "            Column(\n",
    "                *args,\n",
    "                primary_key=col.get(\"primary_key\", False),\n",
    "                nullable=col.get(\"nullable\", True),\n",
    "                autoincrement=col.get(\"autoincrement\", False),\n",
    "                unique=col.get(\"unique\", False),\n",
    "                default=col.get(\"default\", None),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Create new table in the same metadata (which has all reflected tables)\n",
    "    table = Table(table_name, metadata, *column_objs)\n",
    "\n",
    "    if indexes:\n",
    "        for idx in indexes:\n",
    "            Index(idx[\"name\"], *[table.c[col] for col in idx[\"columns\"]])\n",
    "\n",
    "    try:\n",
    "        metadata.create_all(engine)\n",
    "        print(f\"✅ Table '{table_name}' created successfully.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"❌ Error creating table: {e}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    DB_URL = \"postgresql://postgres:iaCkmHPhuyhFLEBDGdwxQGGqlHvdgWJA@yamanote.proxy.rlwy.net:29855/railway\"\n",
    "\n",
    "    columns_metadata = [\n",
    "        {\"name\": \"train_no\", \"type\": \"String\", \"length\": 20, \"nullable\": True,  \"primary_key\": True, \"nullable\": False, },\n",
    "        {\"name\": \"train_name\", \"type\": \"String\", \"length\": 100, \"nullable\": True},\n",
    "        {\"name\": \"source_stn_code\", \"type\": \"String\", \"length\": 10, \"nullable\": True,\n",
    "         \"foreign_key\": \"stations.station_code\"},\n",
    "        {\"name\": \"dstn_stn_code\", \"type\": \"String\", \"length\": 10, \"nullable\": True,\n",
    "         \"foreign_key\": \"stations.station_code\"},\n",
    "        {\"name\": \"from_time\", \"type\": \"String\", \"length\": 10, \"nullable\": True},\n",
    "        {\"name\": \"to_time\", \"type\": \"String\", \"length\": 10, \"nullable\": True},\n",
    "        {\"name\": \"travel_time\", \"type\": \"String\", \"length\": 20, \"nullable\": True},\n",
    "        {\"name\": \"running_days\", \"type\": \"array\", \"nullable\": True},\n",
    "        {\"name\": \"distance\", \"type\": \"String\", \"length\": 20, \"nullable\": True},\n",
    "        {\"name\": \"halts\", \"type\": \"Integer\", \"nullable\": True},\n",
    "    ]\n",
    "\n",
    "    indexes_metadata = [\n",
    "        {\"name\": \"idx_train_no\", \"columns\": [\"train_no\"]},\n",
    "        {\"name\": \"idx_source_stn_code\", \"columns\": [\"source_stn_code\"]},\n",
    "        {\"name\": \"idx_dstn_stn_code\", \"columns\": [\"dstn_stn_code\"]},\n",
    "    ]\n",
    "\n",
    "    create_table_dynamic(DB_URL, \"trains\", columns_metadata, indexes_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c7fac70-09d2-46dc-9317-2054e348c288",
   "metadata": {},
   "outputs": [],
   "source": [
    "JDBC_URL = \"jdbc:postgresql://yamanote.proxy.rlwy.net:29855/railway\"\n",
    "DB_PROPERTIES = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"iaCkmHPhuyhFLEBDGdwxQGGqlHvdgWJA\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6352cfb-aeb2-4841-a25f-8194ee3bf130",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl_commands = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS stations (\n",
    "    station_code VARCHAR(10) PRIMARY KEY,\n",
    "    name VARCHAR(255) NOT NULL,\n",
    "    state VARCHAR(100),\n",
    "    zone VARCHAR(10),\n",
    "    address TEXT,\n",
    "    latitude DOUBLE PRECISION,\n",
    "    longitude DOUBLE PRECISION,\n",
    "    CONSTRAINT station_code_not_empty CHECK (station_code <> ''),\n",
    "    CONSTRAINT latitude_range CHECK (latitude IS NULL OR (latitude >= -90 AND latitude <= 90)),\n",
    "    CONSTRAINT longitude_range CHECK (longitude IS NULL OR (longitude >= -180 AND longitude <= 180))\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS trains (\n",
    "    train_number VARCHAR(10) PRIMARY KEY,\n",
    "    name VARCHAR(255) NOT NULL,\n",
    "    from_station_code VARCHAR(10) NOT NULL REFERENCES stations(station_code) ON DELETE CASCADE,\n",
    "    to_station_code VARCHAR(10) NOT NULL REFERENCES stations(station_code) ON DELETE CASCADE,\n",
    "    distance_km DOUBLE PRECISION NOT NULL CHECK (distance_km >= 0),\n",
    "    duration_h INTEGER NOT NULL CHECK (duration_h >= 0),\n",
    "    duration_m INTEGER NOT NULL CHECK (duration_m >= 0 AND duration_m < 60),\n",
    "    type VARCHAR(50),\n",
    "    first_ac INTEGER DEFAULT 0 CHECK (first_ac >= 0),\n",
    "    second_ac INTEGER DEFAULT 0 CHECK (second_ac >= 0),\n",
    "    third_ac INTEGER DEFAULT 0 CHECK (third_ac >= 0),\n",
    "    first_class INTEGER DEFAULT 0 CHECK (first_class >= 0),\n",
    "    sleeper INTEGER DEFAULT 0 CHECK (sleeper >= 0),\n",
    "    chair_car INTEGER DEFAULT 0 CHECK (chair_car >= 0),\n",
    "    zone VARCHAR(10),\n",
    "    departure_time TIME NOT NULL,\n",
    "    arrival_time TIME NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS schedules (\n",
    "    id BIGINT PRIMARY KEY,\n",
    "    train_number VARCHAR(10) NOT NULL REFERENCES trains(train_number) ON DELETE CASCADE,\n",
    "    train_name VARCHAR(255) NOT NULL,\n",
    "    station_code VARCHAR(10) NOT NULL REFERENCES stations(station_code) ON DELETE CASCADE,\n",
    "    station_name VARCHAR(255) NOT NULL,\n",
    "    day INTEGER NOT NULL CHECK (day >= 1),\n",
    "    arrival_time TIME,\n",
    "    departure_time TIME\n",
    ");\n",
    "\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b207f1dc-2008-4d91-a512-b6570127c084",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "    dbname=\"railway\",\n",
    "    user=DB_PROPERTIES[\"user\"],\n",
    "    password=DB_PROPERTIES[\"password\"],\n",
    "    host=\"yamanote.proxy.rlwy.net\",\n",
    "    port=29855\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dd8588e-cad1-499e-b425-08562a436ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "for ddl in ddl_commands.strip().split(\";\"):\n",
    "    if ddl.strip():\n",
    "        cur.execute(ddl)\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7e89668-8a20-461f-99e3-b7c646838cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_schema = StructType([\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"features\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"geometry\", StructType([\n",
    "                StructField(\"type\", StringType(), True),\n",
    "                StructField(\"coordinates\", ArrayType(DoubleType()), True)\n",
    "            ]), True),\n",
    "            StructField(\"type\", StringType(), True),\n",
    "            StructField(\"properties\", StructType([\n",
    "                StructField(\"state\", StringType(), True),\n",
    "                StructField(\"code\", StringType(), True),\n",
    "                StructField(\"name\", StringType(), True),\n",
    "                StructField(\"zone\", StringType(), True),\n",
    "                StructField(\"address\", StringType(), True)\n",
    "            ]), True)\n",
    "        ])\n",
    "    ), True)\n",
    "])\n",
    "\n",
    "trains_schema = StructType([\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"features\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"geometry\", StructType([\n",
    "                StructField(\"type\", StringType(), True),\n",
    "                StructField(\"coordinates\", ArrayType(ArrayType(DoubleType())), True)\n",
    "            ]), True),\n",
    "            StructField(\"type\", StringType(), True),\n",
    "            StructField(\"properties\", StructType([\n",
    "                StructField(\"third_ac\", IntegerType(), True),\n",
    "                StructField(\"arrival\", StringType(), True),\n",
    "                StructField(\"from_station_code\", StringType(), True),\n",
    "                StructField(\"name\", StringType(), True),\n",
    "                StructField(\"zone\", StringType(), True),\n",
    "                StructField(\"chair_car\", IntegerType(), True),\n",
    "                StructField(\"first_class\", IntegerType(), True),\n",
    "                StructField(\"duration_m\", IntegerType(), True),\n",
    "                StructField(\"sleeper\", IntegerType(), True),\n",
    "                StructField(\"from_station_name\", StringType(), True),\n",
    "                StructField(\"number\", StringType(), True),\n",
    "                StructField(\"departure\", StringType(), True),\n",
    "                StructField(\"return_train\", StringType(), True),\n",
    "                StructField(\"to_station_code\", StringType(), True),\n",
    "                StructField(\"second_ac\", IntegerType(), True),\n",
    "                StructField(\"classes\", StringType(), True),\n",
    "                StructField(\"to_station_name\", StringType(), True),\n",
    "                StructField(\"duration_h\", IntegerType(), True),\n",
    "                StructField(\"type\", StringType(), True),\n",
    "                StructField(\"first_ac\", IntegerType(), True),\n",
    "                StructField(\"distance\", DoubleType(), True)\n",
    "            ]), True)\n",
    "        ])\n",
    "    ), True)\n",
    "])\n",
    "\n",
    "schedules_schema = StructType([\n",
    "    StructField(\"arrival\", StringType(), True),\n",
    "    StructField(\"day\", IntegerType(), True),\n",
    "    StructField(\"train_name\", StringType(), True),\n",
    "    StructField(\"station_name\", StringType(), True),\n",
    "    StructField(\"station_code\", StringType(), True),\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"train_number\", StringType(), True),\n",
    "    StructField(\"departure\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8b05b84-0f44-4052-bc32-624d07881f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+\n",
      "|             type|            features|\n",
      "+-----------------+--------------------+\n",
      "|FeatureCollection|[{{Point, [75.451...|\n",
      "+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stations_df_raw = spark.read.option(\"multiline\", True).schema(stations_schema).json(\"/home/jupyter/data/stations.json\")\n",
    "stations_df_raw.count()\n",
    "stations_df_raw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05bbbcea-3438-4fdd-9f4e-83a1a9dc3f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------+----+--------------------+------------------+-----------------+\n",
      "|station_code|                name|         state|zone|             address|          latitude|        longitude|\n",
      "+------------+--------------------+--------------+----+--------------------+------------------+-----------------+\n",
      "|        BDHL|              Badhal|     Rajasthan| NWR|Kishangarh Renwal...|        27.2520587|       75.4516454|\n",
      "|     XX-BECE|             XX-BECE|          null|null|                null|              null|             null|\n",
      "|     XX-BSPY|             XX-BSPY|          null|null|                null|              null|             null|\n",
      "|     YY-BPLC|             YY-BPLC|          null|null|                null|              null|             null|\n",
      "|         KHH|              KICHHA| Uttar Pradesh| NER|Kichha, Uttar Pra...|28.913427000000002|        79.519746|\n",
      "|        SRKN|            Sherekan|     Rajasthan| NWR|   MDR 89, Rajasthan| 29.55519806064484|74.43499088287354|\n",
      "|        BKKA|            Bhukarka|     Rajasthan| NWR| Bhukarka, Rajasthan|29.238227312503696|74.75103096105158|\n",
      "|         NHR|               Nohar|     Rajasthan| NWR|State Highway 36,...|29.192562738768984|74.77362781763077|\n",
      "|        PRGD|           Perungudi|    Tamil Nadu|  SR| Chennai, Tamil Nadu|         12.975217|        80.231363|\n",
      "|        KNNA|          Khinaniyan|       Haryana| NWR|State Highway 23,...|29.345637263205944| 74.7179570607841|\n",
      "|         JKR|           JAULKHERA|          null|null|                null|         21.838466|        78.216684|\n",
      "|         BYS|             BARSALI|          null|null|                null|         21.909813|78.03136099999999|\n",
      "|          GW|              GIRWAR|          null|null|                null|23.838185999999997|        78.936311|\n",
      "|         DGD|           DANGIDHAR|          null|null|                null|23.871771000000003|        79.019912|\n",
      "|         DOD|             DHODHAR|Madhya Pradesh|  WR|Dhodar, Madhya Pr...|23.770740999999997|75.10354600000001|\n",
      "|         GAJ|          GANESHGANJ|          null|null|                null|         23.895862|        79.074633|\n",
      "|         JNM|Jayanagar Majilpu...|   West Bengal|  ER|Jaynagar, West Be...|         22.168635|        88.415324|\n",
      "|         NSD|           NASIRABAD|     Rajasthan| NWR|Nasirabad, Rajasthan|26.294459999999997|74.71699100000001|\n",
      "|         PHA|            PATHARIA|          null|null|                null|         23.905726|        79.192899|\n",
      "|         NZB|           NIZAMABAD|Andhra Pradesh| SCR|Nizamabad, Andhra...|18.679164999999998|        78.103238|\n",
      "+------------+--------------------+--------------+----+--------------------+------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stations_df = stations_df_raw.selectExpr(\"inline(features)\") \\\n",
    "    .select(\n",
    "        col(\"properties.code\").alias(\"station_code\"),\n",
    "        col(\"properties.name\").alias(\"name\"),\n",
    "        col(\"properties.state\").alias(\"state\"),\n",
    "        col(\"properties.zone\").alias(\"zone\"),\n",
    "        col(\"properties.address\").alias(\"address\"),\n",
    "        col(\"geometry.coordinates\").getItem(1).alias(\"latitude\"),\n",
    "        col(\"geometry.coordinates\").getItem(0).alias(\"longitude\")\n",
    "    )\n",
    "\n",
    "stations_df.count()\n",
    "stations_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a4466f6-7334-4be1-9337-1ba1e6826226",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_df.write.jdbc(JDBC_URL, \"stations\", mode=\"append\", properties=DB_PROPERTIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "379cbfd5-c877-4e15-a94b-b88bc2c7e0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+\n",
      "|             type|            features|\n",
      "+-----------------+--------------------+\n",
      "|FeatureCollection|[{{LineString, [[...|\n",
      "+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trains_df_raw = spark.read.option(\"multiline\", True).schema(trains_schema).json(\"/home/jupyter/data/trains.json\")\n",
    "trains_df_raw.count()\n",
    "trains_df_raw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "324cd571-f837-4f30-89fa-6c9552659f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5208\n",
      "root\n",
      " |-- train_number: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- from_station_code: string (nullable = true)\n",
      " |-- to_station_code: string (nullable = true)\n",
      " |-- distance_km: double (nullable = true)\n",
      " |-- duration_h: integer (nullable = true)\n",
      " |-- duration_m: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- first_ac: integer (nullable = true)\n",
      " |-- second_ac: integer (nullable = true)\n",
      " |-- third_ac: integer (nullable = true)\n",
      " |-- sleeper: integer (nullable = true)\n",
      " |-- chair_car: integer (nullable = true)\n",
      " |-- zone: string (nullable = true)\n",
      " |-- departure_time: timestamp (nullable = true)\n",
      " |-- arrival_time: timestamp (nullable = true)\n",
      "\n",
      "+------------+--------------------+-----------------+---------------+-----------+----------+----------+----+--------+---------+--------+-------+---------+----+-------------------+-------------------+\n",
      "|train_number|                name|from_station_code|to_station_code|distance_km|duration_h|duration_m|type|first_ac|second_ac|third_ac|sleeper|chair_car|zone|     departure_time|       arrival_time|\n",
      "+------------+--------------------+-----------------+---------------+-----------+----------+----------+----+--------+---------+--------+-------+---------+----+-------------------+-------------------+\n",
      "|       04601|Jammu Tawi Udhamp...|              JAT|            UHP|       53.0|         1|        35|DEMU|       0|        0|       0|      0|        0|  NR|1970-01-01 10:40:00|1970-01-01 12:15:00|\n",
      "|       04602|UDHAMPUR JAMMUTAW...|              UHP|            JAT|       53.0|         1|        50|DEMU|       0|        0|       0|      0|        0|  NR|1970-01-01 06:45:00|1970-01-01 08:35:00|\n",
      "|       04603|    JAT UDAHMPUR DMU|              JAT|            UHP|       53.0|         1|        35|DEMU|       0|        0|       0|      0|        0|  NR|1970-01-01 16:15:00|1970-01-01 17:50:00|\n",
      "|       04604|UDHAMPUR JAMMUTAW...|              UHP|            JAT|       53.0|         1|        30|DEMU|       0|        0|       0|      0|        0|  NR|1970-01-01 18:20:00|1970-01-01 19:50:00|\n",
      "|       04728|Mumbai BandraT-Bi...|             BDTS|            BKN|     1212.0|        21|        55|  SF|       0|        1|       1|      1|        0| NWR|1970-01-01 14:35:00|1970-01-01 12:30:00|\n",
      "+------------+--------------------+-----------------+---------------+-----------+----------+----------+----+--------+---------+--------+-------+---------+----+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trains_df = trains_df_raw.selectExpr(\"inline(features)\") \\\n",
    "    .select(\n",
    "        col(\"properties.number\").alias(\"train_number\"),\n",
    "        col(\"properties.name\").alias(\"name\"),\n",
    "        col(\"properties.from_station_code\"),\n",
    "        col(\"properties.to_station_code\"),\n",
    "        col(\"properties.distance\").alias(\"distance_km\"),\n",
    "        col(\"properties.duration_h\"),\n",
    "        col(\"properties.duration_m\"),\n",
    "        col(\"properties.type\"),\n",
    "        col(\"properties.first_ac\"),\n",
    "        col(\"properties.second_ac\"),\n",
    "        col(\"properties.third_ac\"),\n",
    "        col(\"properties.sleeper\"),\n",
    "        col(\"properties.chair_car\"),\n",
    "        col(\"properties.zone\"),\n",
    "        to_timestamp(col(\"properties.departure\"), \"HH:mm:ss\").alias(\"departure_time\"),\n",
    "        to_timestamp(col(\"properties.arrival\"), \"HH:mm:ss\").alias(\"arrival_time\")\n",
    "    )\n",
    "\n",
    "print(trains_df.count())\n",
    "trains_df.printSchema()\n",
    "trains_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62cf00a6-037e-4c9b-8685-ee70b05fade8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5208\n",
      "root\n",
      " |-- train_number: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- from_station_code: string (nullable = true)\n",
      " |-- to_station_code: string (nullable = true)\n",
      " |-- distance_km: double (nullable = true)\n",
      " |-- duration_h: integer (nullable = true)\n",
      " |-- duration_m: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- first_ac: integer (nullable = true)\n",
      " |-- second_ac: integer (nullable = true)\n",
      " |-- third_ac: integer (nullable = true)\n",
      " |-- sleeper: integer (nullable = true)\n",
      " |-- chair_car: integer (nullable = true)\n",
      " |-- zone: string (nullable = true)\n",
      " |-- departure_time: timestamp (nullable = true)\n",
      " |-- arrival_time: timestamp (nullable = true)\n",
      "\n",
      "+------------+--------------------+-----------------+---------------+-----------+----------+----------+----+--------+---------+--------+-------+---------+----+-------------------+-------------------+\n",
      "|train_number|                name|from_station_code|to_station_code|distance_km|duration_h|duration_m|type|first_ac|second_ac|third_ac|sleeper|chair_car|zone|     departure_time|       arrival_time|\n",
      "+------------+--------------------+-----------------+---------------+-----------+----------+----------+----+--------+---------+--------+-------+---------+----+-------------------+-------------------+\n",
      "|       04601|Jammu Tawi Udhamp...|              JAT|            UHP|       53.0|         1|        35|DEMU|       0|        0|       0|      0|        0|  NR|1970-01-01 10:40:00|1970-01-01 12:15:00|\n",
      "|       04602|UDHAMPUR JAMMUTAW...|              UHP|            JAT|       53.0|         1|        50|DEMU|       0|        0|       0|      0|        0|  NR|1970-01-01 06:45:00|1970-01-01 08:35:00|\n",
      "|       04603|    JAT UDAHMPUR DMU|              JAT|            UHP|       53.0|         1|        35|DEMU|       0|        0|       0|      0|        0|  NR|1970-01-01 16:15:00|1970-01-01 17:50:00|\n",
      "|       04604|UDHAMPUR JAMMUTAW...|              UHP|            JAT|       53.0|         1|        30|DEMU|       0|        0|       0|      0|        0|  NR|1970-01-01 18:20:00|1970-01-01 19:50:00|\n",
      "|       04728|Mumbai BandraT-Bi...|             BDTS|            BKN|     1212.0|        21|        55|  SF|       0|        1|       1|      1|        0| NWR|1970-01-01 14:35:00|1970-01-01 12:30:00|\n",
      "+------------+--------------------+-----------------+---------------+-----------+----------+----------+----+--------+---------+--------+-------+---------+----+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trains_df_clean = trains_df.fillna({\n",
    "    \"distance_km\": 0.0,\n",
    "    \"duration_h\": 0,\n",
    "    \"duration_m\": 0,\n",
    "    \"departure_time\": \"1970-01-01 00:00:00\",\n",
    "    \"arrival_time\": \"1970-01-01 00:00:00\"\n",
    "})\n",
    "print(trains_df.count())\n",
    "trains_df.printSchema()\n",
    "trains_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7c4d549-1964-42db-a43b-2391f6f3ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "trains_df.write.jdbc(JDBC_URL, \"trains\", mode=\"append\", properties=DB_PROPERTIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "378ee325-b017-4e71-ac12-b2c1d0d71665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+--------------------+-------------------+------------+------+------------+---------+\n",
      "| arrival|day|          train_name|       station_name|station_code|    id|train_number|departure|\n",
      "+--------+---+--------------------+-------------------+------------+------+------------+---------+\n",
      "|    None|  1|Falaknuma Lingamp...|KACHEGUDA FALAKNUMA|          FM|302214|       47154| 07:55:00|\n",
      "|    None|  1|Thrissur Guruvayu...|            THRISUR|         TCR|281458|       56044| 18:55:00|\n",
      "|    None|  1|Porbandar Muzaffa...|          PORBANDAR|         PBR|309335|       19269| 15:05:00|\n",
      "|    None|  1|  RAIPUR ITWARI PASS|          RAIPUR JN|           R|283774|       58205| 13:30:00|\n",
      "|    None|  1|  Gomoh-Asansol MEMU|           GOMOH JN|         GMO|319937|       63542| 07:20:00|\n",
      "|    None|  1|Rai Bareli-Mumbai...|      RAE BARELI JN|         RBL|520605|       21068| 16:15:00|\n",
      "|    None|  1|Subramanya Road M...|   Subrahmanya Road|        SBHR|335930|       56646| 13:25:00|\n",
      "|    None|  1|Ernakulam Alappuz...|       ERNAKULAM JN|         ERS|282535|       56379| 07:30:00|\n",
      "|    None|  1|Salempur Barhaj P...|        SALEMPUR JN|         SRU|336174|       55145| 18:40:00|\n",
      "|    None|  1| Bir Khandwa Shuttle|                Bir|         BIR| 17022|       51688| 15:15:00|\n",
      "|    None|  1|Asansol-Bokaro St...|         ASANSOL JN|         ASN|336193|       63596| 17:50:00|\n",
      "|    None|  1|Tanakpur Aishbagh...|           Tanakpur|         TPU|282465|       52228| 18:40:00|\n",
      "|    None|  1|Botad Bhavnagar P...|           BOTAD JN|         BTD|336178|       59227| 21:10:00|\n",
      "|    None|  1|     Harihar Express|     MUZAFFARPUR JN|         MFP|168737|       14523| 07:25:00|\n",
      "|    None|  1|Birmitrapur Barsu...|       Biramitrapur|        BRMP|336277|       58151| 07:10:00|\n",
      "|    None|  1|Patan Mehsana Pas...|              PATAN|         PTN|299325|       59478| 17:05:00|\n",
      "|    None|  1|  Sealdah-Gede Local|    KOLKATA SEALDAH|        SDAH|336057|       31929| 20:32:00|\n",
      "|06:49:00|  1|Pipar Road -Jodhp...|      PIPAR ROAD JN|         PPR| 11813|       04857| 06:50:00|\n",
      "|    None|  1|Buxar Mughal Sara...|              BUXAR|         BXR|318164|       53215| 09:15:00|\n",
      "|    None|  1| Howrah Panskura EMU|          HOWRAH JN|         HWH|299882|       38453| 17:50:00|\n",
      "+--------+---+--------------------+-------------------+------------+------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schedules_df = spark.read.option(\"multiline\", True).schema(schedules_schema).json(\"/home/jupyter/data/schedules.json\")\n",
    "schedules_df.count()\n",
    "schedules_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23ac4497-afab-4e88-b38f-819d4a17ba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedules_df = schedules_df \\\n",
    "    .withColumn(\"arrival_time\", to_timestamp(col(\"arrival\"), \"HH:mm:ss\")) \\\n",
    "    .withColumn(\"departure_time\", to_timestamp(col(\"departure\"), \"HH:mm:ss\")) \\\n",
    "    .select(\n",
    "        col(\"id\"),\n",
    "        col(\"train_number\"),\n",
    "        col(\"station_code\"),\n",
    "        col(\"station_name\"),\n",
    "        col(\"day\"),\n",
    "        col(\"arrival_time\"),\n",
    "        col(\"departure_time\"),\n",
    "        col(\"train_name\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "004a5582-0e0b-4c9e-9d5d-6eb285e71321",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedules_df = schedules_df.alias(\"s\") \\\n",
    "    .join(trains_df.select(\"train_number\", \"name\").alias(\"t\"),\n",
    "          on=\"train_number\", how=\"left\") \\\n",
    "    .withColumn(\"train_name\",\n",
    "                coalesce(col(\"s.train_name\"), col(\"t.name\"))) \\\n",
    "    .drop(col(\"t.name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f2ac465-2dad-4ecd-8569-d56205906a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+------------+-------------------+---+-------------------+-------------------+--------------------+\n",
      "|train_number|    id|station_code|       station_name|day|       arrival_time|     departure_time|          train_name|\n",
      "+------------+------+------------+-------------------+---+-------------------+-------------------+--------------------+\n",
      "|       47154|302214|          FM|KACHEGUDA FALAKNUMA|  1|               null|1970-01-01 07:55:00|Falaknuma Lingamp...|\n",
      "|       56044|281458|         TCR|            THRISUR|  1|               null|1970-01-01 18:55:00|Thrissur Guruvayu...|\n",
      "|       19269|309335|         PBR|          PORBANDAR|  1|               null|1970-01-01 15:05:00|Porbandar Muzaffa...|\n",
      "|       58205|283774|           R|          RAIPUR JN|  1|               null|1970-01-01 13:30:00|  RAIPUR ITWARI PASS|\n",
      "|       63542|319937|         GMO|           GOMOH JN|  1|               null|1970-01-01 07:20:00|  Gomoh-Asansol MEMU|\n",
      "|       21068|520605|         RBL|      RAE BARELI JN|  1|               null|1970-01-01 16:15:00|Rai Bareli-Mumbai...|\n",
      "|       56646|335930|        SBHR|   Subrahmanya Road|  1|               null|1970-01-01 13:25:00|Subramanya Road M...|\n",
      "|       56379|282535|         ERS|       ERNAKULAM JN|  1|               null|1970-01-01 07:30:00|Ernakulam Alappuz...|\n",
      "|       55145|336174|         SRU|        SALEMPUR JN|  1|               null|1970-01-01 18:40:00|Salempur Barhaj P...|\n",
      "|       51688| 17022|         BIR|                Bir|  1|               null|1970-01-01 15:15:00| Bir Khandwa Shuttle|\n",
      "|       63596|336193|         ASN|         ASANSOL JN|  1|               null|1970-01-01 17:50:00|Asansol-Bokaro St...|\n",
      "|       52228|282465|         TPU|           Tanakpur|  1|               null|1970-01-01 18:40:00|Tanakpur Aishbagh...|\n",
      "|       59227|336178|         BTD|           BOTAD JN|  1|               null|1970-01-01 21:10:00|Botad Bhavnagar P...|\n",
      "|       14523|168737|         MFP|     MUZAFFARPUR JN|  1|               null|1970-01-01 07:25:00|     Harihar Express|\n",
      "|       58151|336277|        BRMP|       Biramitrapur|  1|               null|1970-01-01 07:10:00|Birmitrapur Barsu...|\n",
      "|       59478|299325|         PTN|              PATAN|  1|               null|1970-01-01 17:05:00|Patan Mehsana Pas...|\n",
      "|       31929|336057|        SDAH|    KOLKATA SEALDAH|  1|               null|1970-01-01 20:32:00|  Sealdah-Gede Local|\n",
      "|       04857| 11813|         PPR|      PIPAR ROAD JN|  1|1970-01-01 06:49:00|1970-01-01 06:50:00|Pipar Road -Jodhp...|\n",
      "|       53215|318164|         BXR|              BUXAR|  1|               null|1970-01-01 09:15:00|Buxar Mughal Sara...|\n",
      "|       38453|299882|         HWH|          HOWRAH JN|  1|               null|1970-01-01 17:50:00| Howrah Panskura EMU|\n",
      "+------------+------+------------+-------------------+---+-------------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schedules_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "321f8574-5d6f-4545-a11b-32aeef088a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------------+---------------+-----------+----------+----------+----+--------+---------+--------+-------+---------+----+-------------------+-------------------+\n",
      "|train_number|                name|from_station_code|to_station_code|distance_km|duration_h|duration_m|type|first_ac|second_ac|third_ac|sleeper|chair_car|zone|     departure_time|       arrival_time|\n",
      "+------------+--------------------+-----------------+---------------+-----------+----------+----------+----+--------+---------+--------+-------+---------+----+-------------------+-------------------+\n",
      "|       12958|New Delhi-Ahmedab...|             NDLS|            ADI|      934.0|        13|        40| Raj|       1|        1|       1|      0|        0|  WR|1970-01-01 19:55:00|1970-01-01 09:35:00|\n",
      "+------------+--------------------+-----------------+---------------+-----------+----------+----------+----+--------+---------+--------+-------+---------+----+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trains_df_clean.filter(col(\"train_number\") == \"12958\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9697d61f-fc45-4311-8d40-71805eaa88d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import http.client\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, explode, concat_ws, to_timestamp, current_timestamp, date_format, when\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "def fetch_train_info(train_number):\n",
    "    conn = http.client.HTTPSConnection(\"indian-railway-irctc.p.rapidapi.com\")\n",
    "    headers = {\n",
    "        'x-rapidapi-key': \"ea82a60e4amsha48c0c76001fb28p1ac782jsn389a7907946c\",\n",
    "        'x-rapidapi-host': \"indian-railway-irctc.p.rapidapi.com\"\n",
    "    }\n",
    "    endpoint = f\"/api/trains-search/v1/train/{train_number}?isH5=true&client=web\"\n",
    "    conn.request(\"GET\", endpoint, headers=headers)\n",
    "    res = conn.getresponse()\n",
    "    data = res.read()\n",
    "    json_data = json.loads(data.decode(\"utf-8\"))\n",
    "    \n",
    "    trains_list = []\n",
    "    if \"body\" in json_data and len(json_data[\"body\"]) > 0 and \"trains\" in json_data[\"body\"][0]:\n",
    "        trains_list = json_data[\"body\"][0][\"trains\"]\n",
    "        \n",
    "    # Normalize list fields to strings to avoid PySparkTypeError\n",
    "    for train in trains_list:\n",
    "        if \"journeyClasses\" in train and isinstance(train[\"journeyClasses\"], list):\n",
    "            train[\"journeyClasses\"] = \",\".join(train[\"journeyClasses\"])\n",
    "        if \"train_type\" in train and isinstance(train[\"train_type\"], list):\n",
    "            train[\"train_type\"] = \",\".join(train[\"train_type\"])\n",
    "    return trains_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7669ef42-421b-404e-a3d0-1ee7b54f80fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'destination': 'Patna Jn',\n",
      "  'journeyClasses': 'SL',\n",
      "  'origin': 'Firozpur Cant',\n",
      "  'runningOn': 'NNNYNNY',\n",
      "  'schedule': [{'arrivalTime': '--',\n",
      "                'boardingDisabled': 'false',\n",
      "                'dayCount': '1',\n",
      "                'departureTime': '15:10',\n",
      "                'distance': '0',\n",
      "                'haltTime': '--',\n",
      "                'routeNumber': '1',\n",
      "                'stationCode': 'FZR',\n",
      "                'stationName': 'Firozpur Cant',\n",
      "                'stnSerialNumber': '1'},\n",
      "               {'arrivalTime': '16:00',\n",
      "                'boardingDisabled': 'false',\n",
      "                'dayCount': '1',\n",
      "                'departureTime': '16:02',\n",
      "                'distance': '56',\n",
      "                'haltTime': '02:00',\n",
      "                'routeNumber': '1',\n",
      "                'stationCode': 'MOGA',\n",
      "                'stationName': 'Moga',\n",
      "                'stnSerialNumber': '2'},\n",
      "               {'arrivalTime': '17:00',\n",
      "                'boardingDisabled': 'false',\n",
      "                'dayCount': '1',\n",
      "                'departureTime': '17:30',\n",
      "                'distance': '124',\n",
      "                'haltTime': '30:00',\n",
      "                'routeNumber': '1',\n",
      "                'stationCode': 'LDH',\n",
      "                'stationName': 'Ludhiana Jn',\n",
      "                'stnSerialNumber': '3'},\n",
      "               {'arrivalTime': '19:50',\n",
      "                'boardingDisabled': 'false',\n",
      "                'dayCount': '1',\n",
      "                'departureTime': '20:00',\n",
      "                'distance': '238',\n",
      "                'haltTime': '10:00',\n",
      "                'routeNumber': '1',\n",
      "                'stationCode': 'UMB',\n",
      "                'stationName': 'Ambala Cant Jn',\n",
      "                'stnSerialNumber': '5'},\n",
      "               {'arrivalTime': '20:50',\n",
      "                'boardingDisabled': 'false',\n",
      "                'dayCount': '1',\n",
      "                'departureTime': '20:52',\n",
      "                'distance': '289',\n",
      "                'haltTime': '02:00',\n",
      "                'routeNumber': '1',\n",
      "                'stationCode': 'YJUD',\n",
      "                'stationName': 'Yamunanagar Jud',\n",
      "                'stnSerialNumber': '6'},\n",
      "               {'arrivalTime': '21:20',\n",
      "                'boardingDisabled': 'false',\n",
      "                'dayCount': '1',\n",
      "                'departureTime': '21:30',\n",
      "                'distance': '319',\n",
      "                'haltTime': '10:00',\n",
      "                'routeNumber': '1',\n",
      "                'stationCode': 'SRE',\n",
      "                'stationName': 'Saharanpur',\n",
      "                'stnSerialNumber': '7'},\n",
      "               {'arrivalTime': '00:45',\n",
      "                'boardingDisabled': 'false',\n",
      "                'dayCount': '2',\n",
      "                'departureTime': '00:55',\n",
      "                'distance': '512',\n",
      "                'haltTime': '10:00',\n",
      "                'routeNumber': '1',\n",
      "                'stationCode': 'MB',\n",
      "                'stationName': 'Moradabad',\n",
      "                'stnSerialNumber': '8'},\n",
      "               {'arrivalTime': '02:25',\n",
      "                'boardingDisabled': 'false',\n",
      "                'dayCount': '2',\n",
      "                'departureTime': '02:27',\n",
      "                'distance': '603',\n",
      "                'haltTime': '02:00',\n",
      "                'routeNumber': '1',\n",
      "                'stationCode': 'BE',\n",
      "                'stationName': 'Bareilly',\n",
      "                'stnSerialNumber': '9'},\n",
      "               {'arrivalTime': '06:30',\n",
      "                'boardingDisabled': 'false',\n",
      "                'dayCount': '2',\n",
      "                'departureTime': '06:40',\n",
      "                'distance': '838',\n",
      "                'haltTime': '10:00',\n",
      "                'routeNumber': '1',\n",
      "                'stationCode': 'LKO',\n",
      "                'stationName': 'Lucknow Nr',\n",
      "                'stnSerialNumber': '10'},\n",
      "               {'arrivalTime': '08:20',\n",
      "                'boardingDisabled': 'false',\n",
      "                'dayCount': '2',\n",
      "                'departureTime': '08:22',\n",
      "                'distance': '915',\n",
      "                'haltTime': '02:00',\n",
      "                'routeNumber': '1',\n",
      "                'stationCode': 'RBL',\n",
      "                'stationName': 'Rae Bareli Jn',\n",
      "                'stnSerialNumber': '11'},\n",
      "               {'arrivalTime': '10:05',\n",
      "                'boardingDisabled': 'false',\n",
      "                'dayCount': '2',\n",
      "                'departureTime': '10:10',\n",
      "                'distance': '1011',\n",
      "                'haltTime': '05:00',\n",
      "                'routeNumber': '1',\n",
      "                'stationCode': 'MBDP',\n",
      "                'stationName': 'Ma Belhadevi Dp',\n",
      "                'stnSerialNumber': '12'},\n",
      "               {'arrivalTime': '12:30',\n",
      "                'boardingDisabled': 'false',\n",
      "                'dayCount': '2',\n",
      "                'departureTime': '12:40',\n",
      "                'distance': '1139',\n",
      "                'haltTime': '10:00',\n",
      "                'routeNumber': '1',\n",
      "                'stationCode': 'BSB',\n",
      "                'stationName': 'Varanasi Jn',\n",
      "                'stnSerialNumber': '13'},\n",
      "               {'arrivalTime': '13:30',\n",
      "                'boardingDisabled': 'false',\n",
      "                'dayCount': '2',\n",
      "                'departureTime': '13:40',\n",
      "                'distance': '1155',\n",
      "                'haltTime': '10:00',\n",
      "                'routeNumber': '1',\n",
      "                'stationCode': 'DDU',\n",
      "                'stationName': 'Dd Upadhyaya Jn',\n",
      "                'stnSerialNumber': '14'},\n",
      "               {'arrivalTime': '14:50',\n",
      "                'boardingDisabled': 'false',\n",
      "                'dayCount': '2',\n",
      "                'departureTime': '14:52',\n",
      "                'distance': '1250',\n",
      "                'haltTime': '02:00',\n",
      "                'routeNumber': '1',\n",
      "                'stationCode': 'BXR',\n",
      "                'stationName': 'Buxar',\n",
      "                'stnSerialNumber': '15'},\n",
      "               {'arrivalTime': '15:40',\n",
      "                'boardingDisabled': 'false',\n",
      "                'dayCount': '2',\n",
      "                'departureTime': '15:42',\n",
      "                'distance': '1318',\n",
      "                'haltTime': '02:00',\n",
      "                'routeNumber': '1',\n",
      "                'stationCode': 'ARA',\n",
      "                'stationName': 'Ara Jn',\n",
      "                'stnSerialNumber': '16'},\n",
      "               {'arrivalTime': '16:20',\n",
      "                'boardingDisabled': 'false',\n",
      "                'dayCount': '2',\n",
      "                'departureTime': '16:22',\n",
      "                'distance': '1358',\n",
      "                'haltTime': '02:00',\n",
      "                'routeNumber': '1',\n",
      "                'stationCode': 'DNR',\n",
      "                'stationName': 'Danapur',\n",
      "                'stnSerialNumber': '17'},\n",
      "               {'arrivalTime': '18:00',\n",
      "                'boardingDisabled': 'false',\n",
      "                'dayCount': '2',\n",
      "                'departureTime': '--',\n",
      "                'distance': '1367',\n",
      "                'haltTime': '--',\n",
      "                'routeNumber': '1',\n",
      "                'stationCode': 'PNBE',\n",
      "                'stationName': 'Patna Jn',\n",
      "                'stnSerialNumber': '18'}],\n",
      "  'stationFrom': 'FZR',\n",
      "  'stationTo': 'PNBE',\n",
      "  'trainName': 'Pnbe Summer Spl',\n",
      "  'trainNumber': '04602',\n",
      "  'train_type': 'ST,SP'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(fetch_train_info(\"04602\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5d05fe13-2c83-4b40-abb6-3a763401f3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13-08-2025\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_date = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "print(current_date)  # Example: 13-08-2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2ca58156-7e22-4748-b823-48b92cb631da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------------+---------------+-----------+----------+----------+----+--------+---------+--------+-------+---------+----+-------------------+-------------------+\n",
      "|train_number|                name|from_station_code|to_station_code|distance_km|duration_h|duration_m|type|first_ac|second_ac|third_ac|sleeper|chair_car|zone|     departure_time|       arrival_time|\n",
      "+------------+--------------------+-----------------+---------------+-----------+----------+----------+----+--------+---------+--------+-------+---------+----+-------------------+-------------------+\n",
      "|       04601|Jammu Tawi Udhamp...|              JAT|            UHP|       53.0|         1|        35|DEMU|       0|        0|       0|      0|        0|  NR|1970-01-01 10:40:00|1970-01-01 12:15:00|\n",
      "|       04602|UDHAMPUR JAMMUTAW...|              UHP|            JAT|       53.0|         1|        50|DEMU|       0|        0|       0|      0|        0|  NR|1970-01-01 06:45:00|1970-01-01 08:35:00|\n",
      "+------------+--------------------+-----------------+---------------+-----------+----------+----------+----+--------+---------+--------+-------+---------+----+-------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trains_df_clean.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "61641278-abf9-4ff8-ad1e-762d181a86d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_train(train_number: str, date_str: str):\n",
    "    \"\"\"Call trackTrain API and return parsed JSON.\"\"\"\n",
    "    url = f\"{BASE_URL}/trackTrain\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\"trainNumber\": train_number, \"date\": date_str}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload, timeout=8)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.RequestException:\n",
    "        return None\n",
    "\n",
    "def parse_train_status(train_number, api_data):\n",
    "    \"\"\"Extract last_crossed, next_upcoming, and delay in minutes.\"\"\"\n",
    "    if not api_data or not api_data.get(\"success\"):\n",
    "        return Row(\n",
    "            train_number=train_number,\n",
    "            last_crossed=None,\n",
    "            next_upcoming=None,\n",
    "            delay_minutes=0\n",
    "        )\n",
    "\n",
    "    stations = api_data.get(\"data\", [])\n",
    "\n",
    "    # Find last crossed station\n",
    "    last_crossed = None\n",
    "    for s in stations:\n",
    "        if s.get(\"status\") == \"crossed\":\n",
    "            last_crossed = s.get(\"station\")\n",
    "\n",
    "    # Find first upcoming station\n",
    "    next_upcoming = None\n",
    "    delay_minutes = 0\n",
    "    for s in stations:\n",
    "        if s.get(\"status\") == \"upcoming\":\n",
    "            next_upcoming = s.get(\"station\")\n",
    "            delay_str = s.get(\"delay\", \"\")\n",
    "            try:\n",
    "                delay_minutes = int(delay_str) if delay_str else 0\n",
    "            except ValueError:\n",
    "                delay_minutes = 0\n",
    "            break\n",
    "\n",
    "    return Row(\n",
    "        train_number=train_number,\n",
    "        last_crossed=last_crossed,\n",
    "        next_upcoming=next_upcoming,\n",
    "        delay_minutes=delay_minutes\n",
    "    )\n",
    "\n",
    "def process_partition(partition):\n",
    "    \"\"\"Run API calls for each train in a Spark partition.\"\"\"\n",
    "    date_str = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "    for row in partition:\n",
    "        train_number = str(row.train_number)\n",
    "        api_data = track_train(train_number, date_str)\n",
    "        yield parse_train_status(train_number, api_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8be39e2e-6df3-44f3-8839-d067677af05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_status_df = trains_df_clean.rdd.mapPartitions(process_partition).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c9c120-a05c-4fe4-a578-def3173730f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_status_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3ade5685-8011-4a82-9e02-e445e7bf8ec5",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o452.jdbc.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 101.0 failed 1 times, most recent failure: Lost task 0.0 in stage 101.0 (TID 98) (d1bd379c1917 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO schedules (\"train_number\",\"id\",\"station_code\",\"station_name\",\"day\",\"arrival_time\",\"departure_time\",\"train_name\") VALUES (('08004'),('633'::int4),('PANP'),('PANDAVAPURA'),('1'::int4),('1970-01-01 00:50:00+00'::time),('1970-01-01 00:50:00+00'::time),('Mysore - Howrah Special')) was aborted: ERROR: duplicate key value violates unique constraint \"schedules_pkey\"\n  Detail: Key (id)=(633) already exists.  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2153)\n\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1503)\n\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1528)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:566)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:888)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:912)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:708)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:868)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:867)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"schedules_pkey\"\n  Detail: Key (id)=(633) already exists.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n\t... 21 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:867)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:757)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO schedules (\"train_number\",\"id\",\"station_code\",\"station_name\",\"day\",\"arrival_time\",\"departure_time\",\"train_name\") VALUES (('08004'),('633'::int4),('PANP'),('PANDAVAPURA'),('1'::int4),('1970-01-01 00:50:00+00'::time),('1970-01-01 00:50:00+00'::time),('Mysore - Howrah Special')) was aborted: ERROR: duplicate key value violates unique constraint \"schedules_pkey\"\n  Detail: Key (id)=(633) already exists.  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2153)\n\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1503)\n\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1528)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:566)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:888)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:912)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:708)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:868)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:867)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"schedules_pkey\"\n  Detail: Key (id)=(633) already exists.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n\t... 21 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mschedules_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mJDBC_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mschedules\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDB_PROPERTIES\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/spark/python/pyspark/sql/readwriter.py:1340\u001b[0m, in \u001b[0;36mDataFrameWriter.jdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[1;32m   1339\u001b[0m     jprop\u001b[38;5;241m.\u001b[39msetProperty(k, properties[k])\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o452.jdbc.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 101.0 failed 1 times, most recent failure: Lost task 0.0 in stage 101.0 (TID 98) (d1bd379c1917 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO schedules (\"train_number\",\"id\",\"station_code\",\"station_name\",\"day\",\"arrival_time\",\"departure_time\",\"train_name\") VALUES (('08004'),('633'::int4),('PANP'),('PANDAVAPURA'),('1'::int4),('1970-01-01 00:50:00+00'::time),('1970-01-01 00:50:00+00'::time),('Mysore - Howrah Special')) was aborted: ERROR: duplicate key value violates unique constraint \"schedules_pkey\"\n  Detail: Key (id)=(633) already exists.  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2153)\n\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1503)\n\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1528)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:566)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:888)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:912)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:708)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:868)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:867)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"schedules_pkey\"\n  Detail: Key (id)=(633) already exists.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n\t... 21 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:867)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:757)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO schedules (\"train_number\",\"id\",\"station_code\",\"station_name\",\"day\",\"arrival_time\",\"departure_time\",\"train_name\") VALUES (('08004'),('633'::int4),('PANP'),('PANDAVAPURA'),('1'::int4),('1970-01-01 00:50:00+00'::time),('1970-01-01 00:50:00+00'::time),('Mysore - Howrah Special')) was aborted: ERROR: duplicate key value violates unique constraint \"schedules_pkey\"\n  Detail: Key (id)=(633) already exists.  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2153)\n\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1503)\n\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1528)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:566)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:888)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:912)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1739)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:708)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:868)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:867)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"schedules_pkey\"\n  Detail: Key (id)=(633) already exists.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n\t... 21 more\n"
     ]
    }
   ],
   "source": [
    "schedules_df.write.jdbc(JDBC_URL, \"schedules\", mode=\"append\", properties=DB_PROPERTIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2922652-d2cc-4d81-bf7c-01fe078b9bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c422d44e-1089-4e0c-9788-d08eef2669cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
